{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Notebook 1.3: Preprocessing\n",
    "\n",
    "**LSE MY459: Computational Text Analysis and Large Language Models** (WT 2026)\n",
    "\n",
    "**Ryan H√ºbert**\n",
    "\n",
    "This notebook covers preprocessing texts and creating document feature matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory management\n",
    "\n",
    "We begin with some directory management to specify the file path to the folder on your computer where you wish to store data for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sdir = os.path.join(os.path.expanduser(\"~\"), \"LSE-MY459-WT26\", \"SeminarWeek02\") # or whatever path you want\n",
    "if not os.path.exists(sdir):\n",
    "    os.mkdir(sdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will examine a corpus of U.S. President Donald Trump's tweets from January 2017 through June 2018. These are contained in a JSON file called `trump-tweets.json` available on the course GitHub page. The following code chunk will download this file and save to `sdir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Where is the remote file?\n",
    "rfile = \"https://raw.githubusercontent.com/lse-my459/data/master/trump-tweets.json\"\n",
    "\n",
    "# Where will we store it locally?\n",
    "lfile = os.path.join(sdir, os.path.basename(rfile))\n",
    "\n",
    "# Check if you have the file yet and if not, download it to correct location\n",
    "if not os.path.exists(lfile):\n",
    "    r = requests.get(rfile) # make GET request for the remote file\n",
    "    r.raise_for_status()    # raise exception if there's an HTTP error\n",
    "    \n",
    "    # Write the raw bytes received from the server to the local file path\n",
    "    with open(lfile, \"wb\") as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with collections of texts\n",
    "\n",
    "We begin by loading the tweets from the downloaded file. The file is a JSON file, but it is uses a layout that is somewhat unusual. In particular, each tweet is contained within a JSON object stored on a new line in the file. You should familiarise yourself with JSON formats by reading <https://en.wikipedia.org/wiki/JSON>. You should also familiarise yourself with Python's `json` module by reviewing parts of the documentation at <https://docs.python.org/3/library/json.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a list to hold data for each tweet\n",
    "tweets = []\n",
    "\n",
    "# Read the data line by line, storing each line as a new element in the `tweets` list\n",
    "with open(lfile, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip() # remove whitespace at beginning/end\n",
    "        if line: # if line object is not empty\n",
    "            tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first tweet in this list of tweets, just to get a sense for how the data looks. You will see that each tweet is represented by a `dict` object, where each key-value pair represents a specific piece of information relating to the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': ['Fri Jun 22 19:40:20 +0000 2018'],\n",
       " 'id': [1.01024612682035e+18],\n",
       " 'id_str': ['1010246126820347906'],\n",
       " 'full_text': ['We are gathered today to hear directly from the AMERICAN VICTIMS of ILLEGAL IMMIGRATION. These are the American Citizens permanently separated from their loved ones b/c they were killed by criminal illegal aliens. These are the families the media ignores...https://t.co/ZjXESYAcjY'],\n",
       " 'truncated': [False],\n",
       " 'display_text_range': [[0], [280]],\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'urls': [{'url': ['https://t.co/ZjXESYAcjY'],\n",
       "    'expanded_url': ['https://www.pscp.tv/w/bf1GFzFvTlFsTFJub1dwUXd8MWpNSmdFVll5ZUFLTAWuHc0BMMKeCOoDRCPmtIftVLaFLQVwfSLoC_C0SbzX?t=9m9s'],\n",
       "    'display_url': ['pscp.tv/w/bf1GFzFvTlFs‚Ä¶'],\n",
       "    'indices': [[257], [280]]}]},\n",
       " 'source': ['<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'],\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'user': {'id': [25073877],\n",
       "  'id_str': ['25073877'],\n",
       "  'name': ['Donald J. Trump'],\n",
       "  'screen_name': ['realDonaldTrump'],\n",
       "  'location': ['Washington, DC'],\n",
       "  'description': ['45th President of the United States of Americaüá∫üá∏'],\n",
       "  'url': ['https://t.co/OMxB0x7xC5'],\n",
       "  'entities': {'url': {'urls': [{'url': ['https://t.co/OMxB0x7xC5'],\n",
       "      'expanded_url': ['http://www.Instagram.com/realDonaldTrump'],\n",
       "      'display_url': ['Instagram.com/realDonaldTrump'],\n",
       "      'indices': [[0], [23]]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': [False],\n",
       "  'followers_count': [53101783],\n",
       "  'friends_count': [47],\n",
       "  'listed_count': [89807],\n",
       "  'created_at': ['Wed Mar 18 13:46:38 +0000 2009'],\n",
       "  'favourites_count': [25],\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': [True],\n",
       "  'verified': [True],\n",
       "  'statuses_count': [38073],\n",
       "  'lang': ['en'],\n",
       "  'contributors_enabled': [False],\n",
       "  'is_translator': [False],\n",
       "  'is_translation_enabled': [True],\n",
       "  'profile_background_color': ['6D5C18'],\n",
       "  'profile_background_image_url': ['http://abs.twimg.com/images/themes/theme1/bg.png'],\n",
       "  'profile_background_image_url_https': ['https://abs.twimg.com/images/themes/theme1/bg.png'],\n",
       "  'profile_background_tile': [True],\n",
       "  'profile_image_url': ['http://pbs.twimg.com/profile_images/874276197357596672/kUuht00m_normal.jpg'],\n",
       "  'profile_image_url_https': ['https://pbs.twimg.com/profile_images/874276197357596672/kUuht00m_normal.jpg'],\n",
       "  'profile_banner_url': ['https://pbs.twimg.com/profile_banners/25073877/1530156698'],\n",
       "  'profile_link_color': ['1B95E0'],\n",
       "  'profile_sidebar_border_color': ['BDDCAD'],\n",
       "  'profile_sidebar_fill_color': ['C5CEC0'],\n",
       "  'profile_text_color': ['333333'],\n",
       "  'profile_use_background_image': [True],\n",
       "  'has_extended_profile': [False],\n",
       "  'default_profile': [False],\n",
       "  'default_profile_image': [False],\n",
       "  'following': [False],\n",
       "  'follow_request_sent': [False],\n",
       "  'notifications': [False],\n",
       "  'translator_type': ['regular']},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': [False],\n",
       " 'retweet_count': [30514],\n",
       " 'favorite_count': [89162],\n",
       " 'favorited': [False],\n",
       " 'retweeted': [False],\n",
       " 'possibly_sensitive': [False],\n",
       " 'lang': ['en']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two pieces of information that are quite useful, the text of the tweet and the date and time of the tweet, let's just look at those pieces of information for the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fri Jun 22 19:40:20 +0000 2018']\n",
      "['We are gathered today to hear directly from the AMERICAN VICTIMS of ILLEGAL IMMIGRATION. These are the American Citizens permanently separated from their loved ones b/c they were killed by criminal illegal aliens. These are the families the media ignores...https://t.co/ZjXESYAcjY']\n"
     ]
    }
   ],
   "source": [
    "print(tweets[0][\"created_at\"])\n",
    "print(tweets[0][\"full_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just focus on the text of the tweets and ignore the dates. We'll extract the text from each tweet and save it as an element of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are gathered today to hear directly from the AMERICAN VICTIMS of ILLEGAL IMMIGRATION. These are the American Citizens permanently separated from their loved ones b/c they were killed by criminal illegal aliens. These are the families the media ignores...https://t.co/ZjXESYAcjY\n",
      "Amy Kremer, Women for Trump, was so great on @foxandfriends. Brave and very smart, thank you Amy! @AmyKremer\n",
      "Thank you South Carolina. Now let‚Äôs get out tomorrow and VOTE for @HenryMcMaster! https://t.co/5xlz0wfMfu\n",
      "Just watched @SharkGregNorman on @foxandfriends. Said ‚ÄúPresident is doing a great job. All over the world, people want to come back to the U.S.‚Äù Thank you Greg, and you‚Äôre looking and doing great!\n",
      "Russia continues to say they had nothing to do with Meddling in our Election! Where is the DNC Server, and why didn‚Äôt Shady James Comey and the now disgraced FBI agents take and closely examine it? Why isn‚Äôt Hillary/Russia being looked at? So many questions, so much corruption!\n"
     ]
    }
   ],
   "source": [
    "tweet_texts = [t['full_text'][0] for t in tweets]\n",
    "\n",
    "# Show first few tweets\n",
    "for tweet in tweet_texts[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig into the data a little bit more. Given the source of the dataset, we can expect that there will be many tweets mentioning topics such as immigration or health care. We can search for patterns in strings using the `in` operator or by using `re` functions. Note that since we have a collection of texts, we will need to iterate over each one to find tweets on these topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are gathered today to hear directly from the AMERICAN VICTIMS of ILLEGAL IMMIGRATION. These are the American Citizens permanently separated from their loved ones b/c they were killed by criminal illegal aliens. These are the families the media ignores...https://t.co/ZjXESYAcjY',\n",
       " 'RT @realDonaldTrump: We are gathered today to hear directly from the AMERICAN VICTIMS of ILLEGAL IMMIGRATION. These are the American Citize‚Ä¶',\n",
       " '....If this is done, illegal immigration will be stopped in it‚Äôs tracks - and at very little, by comparison, cost. This is the only real answer - and we must continue to BUILD THE WALL!',\n",
       " 'HOUSE REPUBLICANS SHOULD PASS THE STRONG BUT FAIR IMMIGRATION BILL, KNOWN AS GOODLATTE II, IN THEIR AFTERNOON VOTE TODAY, EVEN THOUGH THE DEMS WON‚ÄôT LET IT PASS IN THE SENATE. PASSAGE WILL SHOW THAT WE WANT STRONG BORDERS &amp; SECURITY WHILE THE DEMS WANT OPEN BORDERS = CRIME.  WIN!',\n",
       " '....Our Immigration policy, laughed at all over the world, is very unfair to all of those people who have gone through the system legally and are waiting on line for years! Immigration must be based on merit - we need people who will help to Make America Great Again!']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five tweets that mention immigration\n",
    "[x for x in tweet_texts if 'immigration' in x.lower()][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `in` operator is a fast way to search, but less powerful than regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are gathered today to hear directly from the AMERICAN VICTIMS of ILLEGAL IMMIGRATION. These are the American Citizens permanently separated from their loved ones b/c they were killed by criminal illegal aliens. These are the families the media ignores...https://t.co/ZjXESYAcjY',\n",
       " 'RT @realDonaldTrump: We are gathered today to hear directly from the AMERICAN VICTIMS of ILLEGAL IMMIGRATION. These are the American Citize‚Ä¶',\n",
       " '....If this is done, illegal immigration will be stopped in it‚Äôs tracks - and at very little, by comparison, cost. This is the only real answer - and we must continue to BUILD THE WALL!',\n",
       " 'HOUSE REPUBLICANS SHOULD PASS THE STRONG BUT FAIR IMMIGRATION BILL, KNOWN AS GOODLATTE II, IN THEIR AFTERNOON VOTE TODAY, EVEN THOUGH THE DEMS WON‚ÄôT LET IT PASS IN THE SENATE. PASSAGE WILL SHOW THAT WE WANT STRONG BORDERS &amp; SECURITY WHILE THE DEMS WANT OPEN BORDERS = CRIME.  WIN!',\n",
       " '....Our Immigration policy, laughed at all over the world, is very unfair to all of those people who have gone through the system legally and are waiting on line for years! Immigration must be based on merit - we need people who will help to Make America Great Again!']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using re.search() instead, although faster to use `in`\n",
    "import re\n",
    "[x for x in tweet_texts if re.search(r'(immigration|immigrant)', x.lower())][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to know the total proportion of tweets mentioning immigrants or immigration, we can do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02353854112778065\n"
     ]
    }
   ],
   "source": [
    "num = len([x for x in tweet_texts if re.search(r'(immigration|immigrant)', x.lower())])\n",
    "den = len(tweet_texts)\n",
    "print(num/den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing a collection of texts in a table\n",
    "\n",
    "Above we have a collection of texts stored in a list. This is a very simple way to store texts. However, when managing large data projects with lots of texts, we might want to use a more \"structured\" data storage system. We will now store the tweet data in a Pandas data frame so that we can access it using standard Pandas data wrangling techniques for tabular data. Since Pandas is the most common module for working with tabular data in Python, we will expect you to be broadly familiar with how to use Pandas tabular objects to do data analysis.\n",
    "\n",
    "The dataframe will contain one column called `text` with the text of each tweet. Each row will be indexed by the date and time the tweet was posted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri Jun 22 19:40:20 +0000 2018</td>\n",
       "      <td>We are gathered today to hear directly from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Jun 28 11:32:09 +0000 2018</td>\n",
       "      <td>Amy Kremer, Women for Trump, was so great on @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue Jun 26 01:35:03 +0000 2018</td>\n",
       "      <td>Thank you South Carolina. Now let‚Äôs get out to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Jun 28 11:38:40 +0000 2018</td>\n",
       "      <td>Just watched @SharkGregNorman on @foxandfriend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Jun 28 11:25:15 +0000 2018</td>\n",
       "      <td>Russia continues to say they had nothing to do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         datetime  \\\n",
       "0  Fri Jun 22 19:40:20 +0000 2018   \n",
       "1  Thu Jun 28 11:32:09 +0000 2018   \n",
       "2  Tue Jun 26 01:35:03 +0000 2018   \n",
       "3  Thu Jun 28 11:38:40 +0000 2018   \n",
       "4  Thu Jun 28 11:25:15 +0000 2018   \n",
       "\n",
       "                                                text  \n",
       "0  We are gathered today to hear directly from th...  \n",
       "1  Amy Kremer, Women for Trump, was so great on @...  \n",
       "2  Thank you South Carolina. Now let‚Äôs get out to...  \n",
       "3  Just watched @SharkGregNorman on @foxandfriend...  \n",
       "4  Russia continues to say they had nothing to do...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tf = pd.DataFrame({\"datetime\" : [x[\"created_at\"][0] for x in tweets], \n",
    "                   \"text\" : [x[\"full_text\"][0] for x in tweets]})\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our corpus is now in tabular form. Let's do a little cleanup. First, let's convert the `datetime` column into a proper date format. Then, we'll sort the dataset by `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuyi\\AppData\\Local\\Temp\\ipykernel_9912\\847946378.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  tf[\"datetime\"] = pd.to_datetime(tf[\"datetime\"])   # Convert to datetime format\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 05:00:10+00:00</td>\n",
       "      <td>TO ALL AMERICANS-\\n#HappyNewYear &amp;amp; many bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 05:39:13+00:00</td>\n",
       "      <td>RT @DanScavino: On behalf of our next #POTUS &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 05:43:23+00:00</td>\n",
       "      <td>RT @Reince: Happy New Year + God's blessings t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 05:44:17+00:00</td>\n",
       "      <td>RT @EricTrump: 2016 was such an incredible yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 06:49:33+00:00</td>\n",
       "      <td>RT @DonaldJTrumpJr: Happy new year everyone. #...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                                               text\n",
       "0 2017-01-01 05:00:10+00:00  TO ALL AMERICANS-\\n#HappyNewYear &amp; many bl...\n",
       "1 2017-01-01 05:39:13+00:00  RT @DanScavino: On behalf of our next #POTUS &...\n",
       "2 2017-01-01 05:43:23+00:00  RT @Reince: Happy New Year + God's blessings t...\n",
       "3 2017-01-01 05:44:17+00:00  RT @EricTrump: 2016 was such an incredible yea...\n",
       "4 2017-01-01 06:49:33+00:00  RT @DonaldJTrumpJr: Happy new year everyone. #..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[\"datetime\"] = pd.to_datetime(tf[\"datetime\"])   # Convert to datetime format\n",
    "tf = tf.sort_values(\"datetime\")\n",
    "tf = tf.reset_index(drop=True)\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the previous code demonstrates, a key advantage of keeping our texts in a DataFrame is that we can (efficiently) perform manipulations across all documents in the corpus. For example, we can quickly extract specific strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3850</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3862</th>\n",
       "      <th>0</th>\n",
       "      <td>AMERICA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "     match         \n",
       "0    0      AMERICA\n",
       "1    0      AMERICA\n",
       "6    0      AMERICA\n",
       "19   0      AMERICA\n",
       "45   0      AMERICA\n",
       "...             ...\n",
       "3769 0      AMERICA\n",
       "3778 0      AMERICA\n",
       "3796 0      AMERICA\n",
       "3850 0      AMERICA\n",
       "3862 0      AMERICA\n",
       "\n",
       "[127 rows x 1 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[\"text\"].str.extractall(r\"(AMERICA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find all the documents containing specific text, and use it to subset our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2017-01-29 21:45:58+00:00</td>\n",
       "      <td>The joint statement of former presidential can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2017-01-29 21:49:32+00:00</td>\n",
       "      <td>...Senators should focus their energies on ISI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2017-02-02 03:55:49+00:00</td>\n",
       "      <td>Do you believe it? The Obama Administration ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>2017-02-19 21:57:01+00:00</td>\n",
       "      <td>My statement as to what's happening in Sweden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>2017-02-20 14:15:42+00:00</td>\n",
       "      <td>Give the public a break - The FAKE NEWS media ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>2018-06-23 16:57:48+00:00</td>\n",
       "      <td>Heading to Nevada to talk trade and immigratio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>2018-06-23 17:05:33+00:00</td>\n",
       "      <td>It‚Äôs very sad that Nancy Pelosi and her sideki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>2018-06-24 15:02:02+00:00</td>\n",
       "      <td>We cannot allow all of these people to invade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3808</th>\n",
       "      <td>2018-06-25 12:36:29+00:00</td>\n",
       "      <td>Such a difference in the media coverage of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3810</th>\n",
       "      <td>2018-06-25 12:54:25+00:00</td>\n",
       "      <td>....If this is done, illegal immigration will ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  \\\n",
       "198  2017-01-29 21:45:58+00:00   \n",
       "199  2017-01-29 21:49:32+00:00   \n",
       "217  2017-02-02 03:55:49+00:00   \n",
       "329  2017-02-19 21:57:01+00:00   \n",
       "330  2017-02-20 14:15:42+00:00   \n",
       "...                        ...   \n",
       "3791 2018-06-23 16:57:48+00:00   \n",
       "3793 2018-06-23 17:05:33+00:00   \n",
       "3799 2018-06-24 15:02:02+00:00   \n",
       "3808 2018-06-25 12:36:29+00:00   \n",
       "3810 2018-06-25 12:54:25+00:00   \n",
       "\n",
       "                                                   text  \n",
       "198   The joint statement of former presidential can...  \n",
       "199   ...Senators should focus their energies on ISI...  \n",
       "217   Do you believe it? The Obama Administration ag...  \n",
       "329   My statement as to what's happening in Sweden ...  \n",
       "330   Give the public a break - The FAKE NEWS media ...  \n",
       "...                                                 ...  \n",
       "3791  Heading to Nevada to talk trade and immigratio...  \n",
       "3793  It‚Äôs very sad that Nancy Pelosi and her sideki...  \n",
       "3799  We cannot allow all of these people to invade ...  \n",
       "3808  Such a difference in the media coverage of the...  \n",
       "3810  ....If this is done, illegal immigration will ...  \n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.loc[tf[\"text\"].str.contains(r\"(?:immigrant|immigration)\"), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will save this corpus in tabular `.csv` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = os.path.join(sdir, \"tweet-corpus.csv\")\n",
    "if not os.path.exists(corpus_file):\n",
    "    tf.to_csv(corpus_file, index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important warning:** try not to open CSV files in other applications (like Microsoft Excel or Apple Numbers). These applications often make changes to your data without your knowledge, which can cause you problems down the road! If you heed this advice, your future self will thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing a text\n",
    "\n",
    "We will now see how to perform standard pre-processing steps covered in lecture. We begin by preprocessing a specific text. Our running example will be the first tweet in the Trump tweet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TO ALL AMERICANS-\\n'\n",
      " '#HappyNewYear &amp; many blessings to you all! Looking forward to a '\n",
      " 'wonderful &amp; prosperous 2017 as we work together to #MAGAüá∫üá∏ '\n",
      " 'https://t.co/UaBFaoDYHe')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "first_tweet = tf[\"text\"].iloc[0]\n",
    "pprint(first_tweet, width = 80) # Limit width for printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising a text\n",
    "\n",
    "When we use the bag of words model to analyse text for whitespace-delimited languages (like English), we tokenise text on the whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TO',\n",
       " 'ALL',\n",
       " 'AMERICANS-',\n",
       " '#HappyNewYear',\n",
       " '&amp;',\n",
       " 'many',\n",
       " 'blessings',\n",
       " 'to',\n",
       " 'you',\n",
       " 'all!',\n",
       " 'Looking',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " '&amp;',\n",
       " 'prosperous',\n",
       " '2017',\n",
       " 'as',\n",
       " 'we',\n",
       " 'work',\n",
       " 'together',\n",
       " 'to',\n",
       " '#MAGAüá∫üá∏',\n",
       " 'https://t.co/UaBFaoDYHe']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = re.split(r\"\\s+\", first_tweet)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up and formatting tokens\n",
    "\n",
    "When you clean and re-format tokens, you have to make some judgements about how to do it. Below are a sequence of cleaning steps based on the kind of data we're working with (Twitter data). In general, the cleaning steps you take should be appropriate to the dataset you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TO',\n",
       " 'ALL',\n",
       " 'AMERICANS',\n",
       " '#HappyNewYear',\n",
       " 'many',\n",
       " 'blessings',\n",
       " 'to',\n",
       " 'you',\n",
       " 'all',\n",
       " 'Looking',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'prosperous',\n",
       " 'as',\n",
       " 'we',\n",
       " 'work',\n",
       " 'together',\n",
       " 'to',\n",
       " '#MAGA']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [re.sub(r\"https?\\://[^ ]+\", \"\", x) for x in tokens] # drop urls\n",
    "tokens = [re.sub(r\"\\&\\#?[A-z]+;\", \"\", x) for x in tokens]    # drop html chars\n",
    "tokens = [re.sub(r\"[‚Äú‚Äù]\", '\"', x) for x in tokens]    # drop curly quotes\n",
    "tokens = [re.sub(r\"[‚Äò‚Äô]\", \"'\", x) for x in tokens]    # drop curly quotes\n",
    "tokens = [re.sub(r\"[‚Äì‚Äî]\", \"-\", x) for x in tokens]    # drop formatted dashes\n",
    "tokens = [re.sub(r\"[^A-z0-9\\-#@':/_\\.\\$% ]\", \"\", x) for x in tokens]    # drop unneeded characters\n",
    "tokens = [re.sub(r\"(^[^A-z0-9#@\\$]|[^A-z0-9%]$)\", \"\", x) for x in tokens]    # drop unneeded characters\n",
    "tokens = [x for x in tokens if all(re.search(r\"[A-z#@\\-]\", y) for y in x)]    # drop non-words\n",
    "tokens = [x for x in tokens if x != \"\"]    # drop empty strings\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising cases\n",
    "\n",
    "In this application, we want to treat words similarly regardless of their capitalisation. For example, the phrase \"MAKE AMERICA GREAT AGAIN\" should be treated as equivalent to \"Make America Great Again\" or even \"make america great again\". To accomplish this you can make all tokens lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'all',\n",
       " 'americans',\n",
       " '#happynewyear',\n",
       " 'many',\n",
       " 'blessings',\n",
       " 'to',\n",
       " 'you',\n",
       " 'all',\n",
       " 'looking',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'a',\n",
       " 'wonderful',\n",
       " 'prosperous',\n",
       " 'as',\n",
       " 'we',\n",
       " 'work',\n",
       " 'together',\n",
       " 'to',\n",
       " '#maga']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [x.lower() for x in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in some situations, you might _not_ want to standardise all the capitalisation. For example, consider the following Tweet, which uses the phrase \"US\" as an abbreviation for United States and the word \"us\". Note that if you make this tweet all lowercase, then US and us are considered the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RT @TravelGov: Continue to notify us of US citizens overseas impacted by '\n",
      " '#HurricaneIrma &amp; #HurricaneJose. https://t.co/EuIpTB144z https://t‚Ä¶')\n"
     ]
    }
   ],
   "source": [
    "pprint(tf[\"text\"].iloc[1568])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "\n",
    "The `nltk` module has a bunch of tools for NLP tasks. For example, if you import `stopwords` from the `nltk.corpus` submodule, you can access a list of common stop words in several languages. You will need to install `nltk` before running the following code. (Also note that the first time you run it, you will need to download `stopwords`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "try: \n",
    "    ## Try to extract list\n",
    "    sw = stopwords.words('english')\n",
    "except LookupError:\n",
    "    ## If not available, download it\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    sw = stopwords.words('english')\n",
    "\n",
    "## Make all the stop words lowercase (since all our tokens are lowercase)\n",
    "sw = [x.lower() for x in sw]\n",
    "\n",
    "## Show the first 10\n",
    "sw[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove all stop words from the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['americans',\n",
       " '#happynewyear',\n",
       " 'many',\n",
       " 'blessings',\n",
       " 'looking',\n",
       " 'forward',\n",
       " 'wonderful',\n",
       " 'prosperous',\n",
       " 'work',\n",
       " 'together',\n",
       " '#maga']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [x for x in tokens if not x in sw]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "The `nltk.stem` submodule has tools available for both stemming and lemmatising texts. Here, we will use the standard English-language version of the Snowball stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['american',\n",
       " '#happynewyear',\n",
       " 'mani',\n",
       " 'bless',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'wonder',\n",
       " 'prosper',\n",
       " 'work',\n",
       " 'togeth',\n",
       " '#maga']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import snowball\n",
    "sstemmer = snowball.SnowballStemmer(\"english\")\n",
    "tokens = [sstemmer.stem(x) for x in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit distance\n",
    "\n",
    "Sometimes you need to calculate the edit distance of two strings. For example, you might use it to find and correct important typos in your corpus. While we will not use it on the Trump tweets, the following code chunk shows you how you can calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "edit_distance(\"kitten\", \"sitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate token frequency\n",
    "\n",
    "Once we have a list of preprocessed tokens, we can then count the number of times each token appears in the list. In the `collections` module, the `Counter` class allows you to count the number of times an element appears in a list. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A': 3, 'b': 2, 'a': 1, 1: 1, 17: 1, 'z': 1})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "a_list = [\"A\", \"a\", \"b\", \"b\", 1, 17, \"A\", \"z\", \"A\"]\n",
    "Counter(a_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can use `Counter` to calculate token frequency in our preprocessed list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'american': 1,\n",
       "         '#happynewyear': 1,\n",
       "         'mani': 1,\n",
       "         'bless': 1,\n",
       "         'look': 1,\n",
       "         'forward': 1,\n",
       "         'wonder': 1,\n",
       "         'prosper': 1,\n",
       "         'work': 1,\n",
       "         'togeth': 1,\n",
       "         '#maga': 1})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_counts = Counter(tokens)\n",
    "tok_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the tweet we're analysing, each token only appears once. But that is not always guaranteed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing a whole corpus\n",
    "\n",
    "The steps above were useful for seeing how an individual text is preprocessed. However, in most applications, you will want to preprocess a whole corpus of documents all at once. Recall that we stored our documents (tweets) as a column of a Pandas `DataFrame`. We'll now add a new column where we'll store preprocess tokens for the documents. We'll do this by applying our preprocessing steps in order.\n",
    "\n",
    "First we tokenise on whitespace using the `.str.split()` method available for Pandas DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [TO, ALL, AMERICANS-, #HappyNewYear, &amp;, ma...\n",
       "1       [RT, @DanScavino:, On, behalf, of, our, next, ...\n",
       "2       [RT, @Reince:, Happy, New, Year, +, God's, ble...\n",
       "3       [RT, @EricTrump:, 2016, was, such, an, incredi...\n",
       "4       [RT, @DonaldJTrumpJr:, Happy, new, year, every...\n",
       "                              ...                        \n",
       "3861    [Today,, we, broke, ground, on, a, plant, that...\n",
       "3862    [AMERICA, IS, OPEN, FOR, BUSINESS!, https://t....\n",
       "3863    [Prior, to, departing, Wisconsin,, I, was, bri...\n",
       "3864    [Before, going, any, further, today,, I, want,...\n",
       "3865    [Six, months, after, our, TAX, CUTS,, more, th...\n",
       "Name: preprocessed, Length: 3866, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[\"preprocessed\"] = tf[\"text\"].str.split(r\"\\s+\")\n",
    "tf[\"preprocessed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll apply a set of formatting rules using the `.apply()` method, which \"applies\" a function to each cell in the \"preprocessed\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [TO, ALL, AMERICANS, #HappyNewYear, many, bles...\n",
       "1    [RT, @DanScavino, On, behalf, of, our, next, #...\n",
       "2    [RT, @Reince, Happy, New, Year, God's, blessin...\n",
       "3    [RT, @EricTrump, was, such, an, incredible, ye...\n",
       "4    [RT, @DonaldJTrumpJr, Happy, new, year, everyo...\n",
       "Name: preprocessed, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Drop urls\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if not re.search(r\"https?\\://.+\", x)])\n",
    "# Drop tokens with punctuation\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if re.search(r\"^[\\#@]?[A-z]\", x)])\n",
    "# Drop any excess punctuation\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [re.sub(r\"[^A-z0-9]+$\", '', x) for x in doc_tokens])\n",
    "# Drop double quotes\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [re.sub(r'[‚Äú‚Äù\"]', '', x) for x in doc_tokens])\n",
    "# Reformat \"curly\" apostrophes and formatted dashes\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [re.sub(r\"[‚Äò‚Äô]\", \"'\", x) for x in doc_tokens])\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [re.sub(r\"[‚Äì‚Äî]\", \"-\", x) for x in doc_tokens])             \n",
    "# Drop empty strings\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if x != \"\"])\n",
    "tf[\"preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will make all the tokens lowercase (even though we know this is not always ideal, see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [to, all, americans, #happynewyear, many, bles...\n",
       "1    [rt, @danscavino, on, behalf, of, our, next, #...\n",
       "2    [rt, @reince, happy, new, year, god's, blessin...\n",
       "3    [rt, @erictrump, was, such, an, incredible, ye...\n",
       "4    [rt, @donaldjtrumpjr, happy, new, year, everyo...\n",
       "Name: preprocessed, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x.lower() for x in doc_tokens])\n",
    "tf[\"preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [americans, #happynewyear, many, blessings, lo...\n",
       "1    [rt, @danscavino, behalf, next, #potus, @teamt...\n",
       "2    [rt, @reince, happy, new, year, god's, blessin...\n",
       "3    [rt, @erictrump, incredible, year, entire, fam...\n",
       "4    [rt, @donaldjtrumpjr, happy, new, year, everyo...\n",
       "Name: preprocessed, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = [x.lower() for x in stopwords.words('english')]\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if not x in sw])\n",
    "tf[\"preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, stem each token using the Snowball stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [american, #happynewyear, mani, bless, look, f...\n",
       "1    [rt, @danscavino, behalf, next, #potus, @teamt...\n",
       "2    [rt, @reince, happi, new, year, god, bless, lo...\n",
       "3    [rt, @erictrump, incred, year, entir, famili, ...\n",
       "4    [rt, @donaldjtrumpjr, happi, new, year, everyo...\n",
       "Name: preprocessed, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import snowball\n",
    "sstemmer = snowball.SnowballStemmer(\"english\")\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [sstemmer.stem(x) if not x[0] in [\"#\", \"@\"] else x for x in doc_tokens])\n",
    "tf[\"preprocessed\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will calculate token frequency for each list of preprocessed tokens using the `Counter` function, and applying it to the column using `.map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'american': 1, '#happynewyear': 1, 'mani': 1,...\n",
       "1    {'rt': 1, '@danscavino': 1, 'behalf': 1, 'next...\n",
       "2    {'rt': 1, '@reince': 1, 'happi': 1, 'new': 1, ...\n",
       "3    {'rt': 1, '@erictrump': 1, 'incred': 1, 'year'...\n",
       "4    {'rt': 1, '@donaldjtrumpjr': 1, 'happi': 1, 'n...\n",
       "Name: term_freqs, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[\"term_freqs\"] = tf[\"preprocessed\"].map(Counter)\n",
    "tf[\"term_freqs\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Above we tokenised on whitespace, which gave us unigrams as our core feature of interest. Of course, we might want to define our features differently and use bigrams, trigrams or other n-grams. Why? We might care about word ordering in our analysis and n-grams give us a crude way to include word order information in our document features. First, let's see how we can extract all the ngrams of a certain size from a single list of tokens representing one document. In this case, we'll extract bigrams from the list of already-processed tokens for Trump's first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american', '#happynewyear'),\n",
       " ('#happynewyear', 'mani'),\n",
       " ('mani', 'bless'),\n",
       " ('bless', 'look'),\n",
       " ('look', 'forward'),\n",
       " ('forward', 'wonder'),\n",
       " ('wonder', 'prosper'),\n",
       " ('prosper', 'work'),\n",
       " ('work', 'togeth'),\n",
       " ('togeth', '#maga')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common use case for ngrams is identifying common collocations that the analyst may want to treat as a single token. Let's extract the top bigrams from the entire corpus and list them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(fake, news)</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(tax, cut)</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(north, korea)</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(unit, state)</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(make, america)</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(america, great)</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(white, hous)</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(witch, hunt)</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(look, forward)</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(great, honor)</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(news, media)</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(prime, minist)</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(rt, @foxandfriends)</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(work, hard)</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(stock, market)</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(crook, hillari)</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(last, night)</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(honor, welcom)</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(men, women)</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(job, job)</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bigram  count\n",
       "0           (fake, news)    213\n",
       "1             (tax, cut)    130\n",
       "2         (north, korea)    114\n",
       "3          (unit, state)    101\n",
       "4        (make, america)     99\n",
       "5       (america, great)     92\n",
       "6          (white, hous)     78\n",
       "7          (witch, hunt)     75\n",
       "8        (look, forward)     71\n",
       "9         (great, honor)     69\n",
       "10         (news, media)     63\n",
       "11       (prime, minist)     56\n",
       "12  (rt, @foxandfriends)     52\n",
       "13          (work, hard)     51\n",
       "14       (stock, market)     48\n",
       "15      (crook, hillari)     48\n",
       "16         (last, night)     43\n",
       "17       (honor, welcom)     43\n",
       "18          (men, women)     43\n",
       "19            (job, job)     40"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bigrams = tf[\"preprocessed\"].apply(lambda x: list(ngrams(x,2))).to_list()\n",
    "top_bigrams = Counter(b for lst in top_bigrams for b in lst)\n",
    "top_bigrams = pd.DataFrame(top_bigrams.items(), columns=[\"bigram\", \"count\"])\n",
    "top_bigrams.sort_values(\"count\", ascending=False)[0:20].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a document feature matrix\n",
    "\n",
    "Now that we have counted the (preprocessed) tokens from each document, we can create a document feature matrix. One way to do this (which is quite easy) is to create a Pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>american</th>\n",
       "      <th>#happynewyear</th>\n",
       "      <th>mani</th>\n",
       "      <th>bless</th>\n",
       "      <th>look</th>\n",
       "      <th>forward</th>\n",
       "      <th>wonder</th>\n",
       "      <th>prosper</th>\n",
       "      <th>work</th>\n",
       "      <th>togeth</th>\n",
       "      <th>...</th>\n",
       "      <th>kremer</th>\n",
       "      <th>@amykremer</th>\n",
       "      <th>@sharkgregnorman</th>\n",
       "      <th>gig</th>\n",
       "      <th>mueller/comey</th>\n",
       "      <th>collusion)...and</th>\n",
       "      <th>groundbreak</th>\n",
       "      <th>discov</th>\n",
       "      <th>newsroom</th>\n",
       "      <th>conscienc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 5949 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   american  #happynewyear  mani  bless  look  forward  wonder  prosper  work  \\\n",
       "0         1              1     1      1     1        1       1        1     1   \n",
       "1         0              1     0      0     0        0       0        0     0   \n",
       "2         0              0     0      1     1        1       0        0     0   \n",
       "3         0              0     0      0     0        0       0        0     0   \n",
       "4         0              0     0      0     0        0       0        0     0   \n",
       "\n",
       "   togeth  ...  kremer  @amykremer  @sharkgregnorman  gig  mueller/comey  \\\n",
       "0       1  ...       0           0                 0    0              0   \n",
       "1       0  ...       0           0                 0    0              0   \n",
       "2       0  ...       0           0                 0    0              0   \n",
       "3       0  ...       0           0                 0    0              0   \n",
       "4       0  ...       0           0                 0    0              0   \n",
       "\n",
       "   collusion)...and  groundbreak  discov  newsroom  conscienc  \n",
       "0                 0            0       0         0          0  \n",
       "1                 0            0       0         0          0  \n",
       "2                 0            0       0         0          0  \n",
       "3                 0            0       0         0          0  \n",
       "4                 0            0       0         0          0  \n",
       "\n",
       "[5 rows x 5949 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataFrame from the Counters\n",
    "dfm = pd.DataFrame(tf[\"term_freqs\"].to_list())\n",
    "# Fill all missing values with zeroes and convert all data to integers\n",
    "dfm = dfm.fillna(0).astype(int)\n",
    "dfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not have any way to know which row corresponds to which tweet. So, we will redefine the `DataFrame` index to be a tweet's date and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.index = tf[\"datetime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine bot the shape of the dfm and then a subset of the rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3866, 5949)\n",
      "                           american  #happynewyear  mani  bless  look  forward\n",
      "datetime                                                                      \n",
      "2017-01-01 05:00:10+00:00         1              1     1      1     1        1\n",
      "2017-01-01 05:39:13+00:00         0              1     0      0     0        0\n",
      "2017-01-01 05:43:23+00:00         0              0     0      1     1        1\n",
      "2017-01-01 05:44:17+00:00         0              0     0      0     0        0\n",
      "2017-01-01 06:49:33+00:00         0              0     0      0     0        0\n",
      "2017-01-01 06:49:49+00:00         0              0     0      0     0        0\n"
     ]
    }
   ],
   "source": [
    "print(dfm.shape)\n",
    "print(dfm.iloc[:6,:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done all this work, we should save it! Again, try to avoid opening this CSV in Excel (or Numbers)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_file = os.path.join(sdir, \"tweet-dfm.csv\")\n",
    "dfm.to_csv(dfm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach above works fine for a relatively small corpus. As the corpus grows, however, representing document feature matrices as Pandas `DataFrame` objects becomes computationally inefficient. Document feature matrices are typically extremely wide (tens of thousands of columns) and mostly sparse (mostly zeroes). `DataFrame` objects are designed for heterogeneous, human-readable tables, not for large, sparse numeric matrices. As a result, they incur substantial memory and performance overhead at scale.\n",
    "\n",
    "That said, using the process above on a relatively small corpus (like the Trump tweets) will yield a `DataFrame` object that is pretty big:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DFM as Pandas DataFrame: 175.5 MB'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_used = dfm.memory_usage(deep=True).sum()\n",
    "\"DFM as Pandas DataFrame: \" + str(round(mem_used / 1024**2, 2)) + \" MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the file size of the DFM we saved as a csv is also big:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DFM as Pandas DataFrame: 44.01 MB'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"DFM as Pandas DataFrame: \" + str(round(os.path.getsize(dfm_file) / 1024**2, 2)) + \" MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisers \n",
    "\n",
    "In many research applications with larger corpuses, it is often a good idea to represent document feature matrices in more efficient ways. The core problem is that DFMs are usually sparse matrices (or arrays) \"where only a few locations in the array have any data, most of the locations are considered as ‚Äúempty‚Äù\" ([Source](https://docs.scipy.org/doc/scipy/tutorial/sparse.html)). As you just saw, even this relatively small corpus created sparse DFMs that were very large (in memory and in storage).\n",
    "\n",
    "A very common approach is to store DFMs as a objects that are optimised for storing sparse arrays. [SciPy](https://scipy.org/) offers a set of tools in its `scipy.sparse` submodule, which are very popular for text. The `sklearn.feature_extraction` submodule offers two options for creating efficient sparse SciPy arrays from the kind of text data we use in this class: `DictVectorizer` and `CountVectorizer`. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, `DictVectorizer` is quite convenient, as it allows us to take our column of `Counter` objects from above and create a sparse array easily as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: initialise a DictVectorizer object, which we'll call `dv`\n",
    "dv = DictVectorizer()\n",
    "# Step 2: create the array from our data\n",
    "dfm_sparse = dv.fit_transform(tf[\"term_freqs\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see below that we have now created a `csr_matrix` object, containing data from a DFM with 3866 rows and 5949 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(3866, 5949)\n"
     ]
    }
   ],
   "source": [
    "print(type(dfm_sparse))\n",
    "print(dfm_sparse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see that this object is _much_ smaller than the Pandas `DataFrame` we made above, even though it contains the same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DFM as csr_matrix: 0.59 MB'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_used = (dfm_sparse.data.nbytes + dfm_sparse.indices.nbytes + dfm_sparse.indptr.nbytes)\n",
    "\"DFM as csr_matrix: \" + str(round(mem_used / 1024**2, 2)) + \" MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That said, it is more awkward to \"see\" the data as a `csr_matrix` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 50274 stored elements and shape (3866, 5949)>\n",
      "  Coords\tValues\n",
      "  (0, 96)\t1.0\n",
      "  (0, 151)\t1.0\n",
      "  (0, 883)\t1.0\n",
      "  (0, 1200)\t1.0\n",
      "  (0, 2533)\t1.0\n",
      "  (0, 3470)\t1.0\n",
      "  (0, 3544)\t1.0\n",
      "  (0, 4404)\t1.0\n",
      "  (0, 5424)\t1.0\n",
      "  (0, 5876)\t1.0\n",
      "  (0, 5881)\t1.0\n",
      "  (1, 96)\t1.0\n",
      "  (1, 209)\t1.0\n",
      "  (1, 381)\t1.0\n",
      "  (1, 660)\t1.0\n",
      "  (1, 881)\t1.0\n",
      "  (1, 1130)\t1.0\n",
      "  (1, 2943)\t1.0\n",
      "  (1, 3881)\t1.0\n",
      "  (1, 4732)\t1.0\n",
      "  (2, 590)\t1.0\n",
      "  (2, 595)\t1.0\n",
      "  (2, 881)\t1.0\n",
      "  (2, 1200)\t1.0\n",
      "  (2, 2533)\t1.0\n",
      "  :\t:\n",
      "  (3864, 2922)\t1.0\n",
      "  (3864, 3570)\t1.0\n",
      "  (3864, 3833)\t1.0\n",
      "  (3864, 3880)\t1.0\n",
      "  (3864, 4236)\t1.0\n",
      "  (3864, 4940)\t1.0\n",
      "  (3864, 4941)\t1.0\n",
      "  (3864, 5422)\t1.0\n",
      "  (3864, 5436)\t1.0\n",
      "  (3864, 5764)\t1.0\n",
      "  (3864, 5925)\t1.0\n",
      "  (3865, 261)\t1.0\n",
      "  (3865, 746)\t1.0\n",
      "  (3865, 1226)\t1.0\n",
      "  (3865, 1681)\t1.0\n",
      "  (3865, 1802)\t1.0\n",
      "  (3865, 3691)\t1.0\n",
      "  (3865, 3752)\t1.0\n",
      "  (3865, 4153)\t1.0\n",
      "  (3865, 4470)\t1.0\n",
      "  (3865, 4514)\t1.0\n",
      "  (3865, 4641)\t1.0\n",
      "  (3865, 4985)\t1.0\n",
      "  (3865, 5315)\t1.0\n",
      "  (3865, 5882)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(dfm_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sparse array format stores each non-zero value from the DFM in a list of values, where each one is associated with a cell, whose index is shown under \"Coords\". For example, the first line above says that in a \"regular\" DFM, there would be a value of 1 in the cell of the matrix in row 0 and column 96. Row 0 corresponds to the first document in the DFM, but what is column 96? \n",
    "\n",
    "Unfortunately, the sparse array object does not store the feature names corresponding to the columns. But, the `DictVectorizer` object we created does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#afghanistan', '#afghanstrategy', '#ahca', ..., 'zoo', 'zte',\n",
       "       'zuker'], shape=(5949,), dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = dv.get_feature_names_out()\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to find out which feature corresponds to column 96, we can look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#happynewyear'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[96]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first document in the DFM (row 0) has the token '#happynewyear' in it one time. You can access the coordinates in the DFM that have non-zero token counts as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0, ..., 3865, 3865, 3865],\n",
       "       shape=(50274,), dtype=int32),\n",
       " array([  96,  151,  883, ..., 4985, 5315, 5882],\n",
       "       shape=(50274,), dtype=int32))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_sparse.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can access the non-zero values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.], shape=(50274,))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_sparse.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to see the non-zero values for the first document in the DFM (row 0), you can index:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 11 stored elements and shape (1, 5949)>\n",
      "  Coords\tValues\n",
      "  (0, 96)\t1.0\n",
      "  (0, 151)\t1.0\n",
      "  (0, 883)\t1.0\n",
      "  (0, 1200)\t1.0\n",
      "  (0, 2533)\t1.0\n",
      "  (0, 3470)\t1.0\n",
      "  (0, 3544)\t1.0\n",
      "  (0, 4404)\t1.0\n",
      "  (0, 5424)\t1.0\n",
      "  (0, 5876)\t1.0\n",
      "  (0, 5881)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(dfm_sparse[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can show the token counts for that row, which look correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#happynewyear', 1),\n",
       " ('#maga', 1),\n",
       " ('american', 1),\n",
       " ('bless', 1),\n",
       " ('forward', 1),\n",
       " ('look', 1),\n",
       " ('mani', 1),\n",
       " ('prosper', 1),\n",
       " ('togeth', 1),\n",
       " ('wonder', 1),\n",
       " ('work', 1)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x[0], int(x[1])) for x in zip(vocabulary[dfm_sparse[0].indices], dfm_sparse[0].data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to go back to a more standard tabular format? For example, for saving, or more \"easy\" previewing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(3866, 5949))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#afghanistan</th>\n",
       "      <th>#afghanstrategy</th>\n",
       "      <th>#ahca</th>\n",
       "      <th>#alconv2017</th>\n",
       "      <th>#amazonwashingtonpost</th>\n",
       "      <th>#america</th>\n",
       "      <th>#americafirst</th>\n",
       "      <th>#americafirstüá∫üá∏#unga</th>\n",
       "      <th>#americanheroes</th>\n",
       "      <th>#anthem</th>\n",
       "      <th>...</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yulin</th>\n",
       "      <th>yuma</th>\n",
       "      <th>zero</th>\n",
       "      <th>zink</th>\n",
       "      <th>zito</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zte</th>\n",
       "      <th>zuker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3862</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3866 rows √ó 5949 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      #afghanistan  #afghanstrategy  #ahca  #alconv2017  \\\n",
       "0              0.0              0.0    0.0          0.0   \n",
       "1              0.0              0.0    0.0          0.0   \n",
       "2              0.0              0.0    0.0          0.0   \n",
       "3              0.0              0.0    0.0          0.0   \n",
       "4              0.0              0.0    0.0          0.0   \n",
       "...            ...              ...    ...          ...   \n",
       "3861           0.0              0.0    0.0          0.0   \n",
       "3862           0.0              0.0    0.0          0.0   \n",
       "3863           0.0              0.0    0.0          0.0   \n",
       "3864           0.0              0.0    0.0          0.0   \n",
       "3865           0.0              0.0    0.0          0.0   \n",
       "\n",
       "      #amazonwashingtonpost  #america  #americafirst  #americafirstüá∫üá∏#unga  \\\n",
       "0                       0.0       0.0            0.0                   0.0   \n",
       "1                       0.0       0.0            0.0                   0.0   \n",
       "2                       0.0       0.0            0.0                   0.0   \n",
       "3                       0.0       0.0            0.0                   0.0   \n",
       "4                       0.0       0.0            0.0                   0.0   \n",
       "...                     ...       ...            ...                   ...   \n",
       "3861                    0.0       0.0            0.0                   0.0   \n",
       "3862                    0.0       0.0            0.0                   0.0   \n",
       "3863                    0.0       0.0            0.0                   0.0   \n",
       "3864                    0.0       0.0            0.0                   0.0   \n",
       "3865                    0.0       0.0            0.0                   0.0   \n",
       "\n",
       "      #americanheroes  #anthem  ...  yrs  yulin  yuma  zero  zink  zito  zone  \\\n",
       "0                 0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1                 0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2                 0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3                 0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4                 0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...               ...      ...  ...  ...    ...   ...   ...   ...   ...   ...   \n",
       "3861              0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3862              0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3863              0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3864              0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3865              0.0      0.0  ...  0.0    0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "      zoo  zte  zuker  \n",
       "0     0.0  0.0    0.0  \n",
       "1     0.0  0.0    0.0  \n",
       "2     0.0  0.0    0.0  \n",
       "3     0.0  0.0    0.0  \n",
       "4     0.0  0.0    0.0  \n",
       "...   ...  ...    ...  \n",
       "3861  0.0  0.0    0.0  \n",
       "3862  0.0  0.0    0.0  \n",
       "3863  0.0  0.0    0.0  \n",
       "3864  0.0  0.0    0.0  \n",
       "3865  0.0  0.0    0.0  \n",
       "\n",
       "[3866 rows x 5949 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dfm_sparse.toarray(), columns=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save sparse matrices using the `save_npz()` function from the `scipy.sparse` submodule. Recall that the sparse matrix object does not contain the feature names. So we need to save two objects: the sparse matrix with the data, and a list of the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse_dfm_file = os.path.join(sdir, 'tweet-dfm-sparse.npz')\n",
    "sparse.save_npz(sparse_dfm_file, dfm_sparse)\n",
    "\n",
    "features_file = os.path.join(sdir, 'tweet-dfm-features.txt')\n",
    "with open(features_file, 'w') as f:\n",
    "    f.write(\"\\n\".join(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, you will need to work with DFMs stored both as Pandas `DataFrame` objects _and_ as sparse array objects. For the most part, you will be able to work with both data types using similar(ish) syntax. Moreover, if your computer can handle it and you do not like working with sparse array objects, you _can_ convert them to Pandas `DataFrame` objects using the process described above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to sparse array\n",
    "\n",
    "Above, we used our own preprocessed `Counter` objects to create a sparse array. This allowed us to have full control over the preprocessing steps. In general, you _should_ fully control the preprocessing steps. However, it is important to know that there is the possibility of applying a different function (`CountVectorizer`) to a collection of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '00am' ... 'Ê≠¥Âè≤ÁöÑ„Å™Êó•Êú¨Ë®™Âïè„ÅØ' 'ÈñìÈÅï„ÅÑ„Å™„Åè' 'ÈùûÂ∏∏„Å´ÈáçË¶Å„Å™ÁÇπ„ÅßË™çË≠ò„Çí‰∏ÄËá¥„Åï„Åõ„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åó„Åü']\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 87103 stored elements and shape (3866, 9226)>\n",
      "  Coords\tValues\n",
      "  (0, 8168)\t4\n",
      "  (0, 697)\t2\n",
      "  (0, 744)\t1\n",
      "  (0, 3743)\t1\n",
      "  (0, 753)\t2\n",
      "  (0, 5068)\t1\n",
      "  (0, 1237)\t1\n",
      "  (0, 9119)\t1\n",
      "  (0, 4928)\t1\n",
      "  (0, 3338)\t1\n",
      "  (0, 8949)\t1\n",
      "  (0, 6421)\t1\n",
      "  (0, 121)\t1\n",
      "  (0, 915)\t1\n",
      "  (0, 8810)\t1\n",
      "  (0, 8956)\t1\n",
      "  (0, 8175)\t1\n",
      "  (0, 5027)\t1\n",
      "  (0, 3995)\t1\n",
      "  (0, 1758)\t1\n",
      "  (0, 8374)\t1\n",
      "  (1, 3743)\t1\n",
      "  (1, 753)\t1\n",
      "  (1, 3995)\t3\n",
      "  (1, 1758)\t2\n",
      "  :\t:\n",
      "  (3864, 1944)\t1\n",
      "  (3864, 4694)\t1\n",
      "  (3865, 3995)\t1\n",
      "  (3865, 1758)\t1\n",
      "  (3865, 5870)\t1\n",
      "  (3865, 766)\t1\n",
      "  (3865, 7982)\t1\n",
      "  (3865, 6015)\t1\n",
      "  (3865, 3780)\t1\n",
      "  (3865, 5367)\t1\n",
      "  (3865, 8060)\t1\n",
      "  (3865, 625)\t1\n",
      "  (3865, 5268)\t1\n",
      "  (3865, 2153)\t1\n",
      "  (3865, 5359)\t1\n",
      "  (3865, 8959)\t1\n",
      "  (3865, 6674)\t1\n",
      "  (3865, 1985)\t1\n",
      "  (3865, 7477)\t1\n",
      "  (3865, 530)\t1\n",
      "  (3865, 7984)\t1\n",
      "  (3865, 6595)\t1\n",
      "  (3865, 1282)\t1\n",
      "  (3865, 6899)\t1\n",
      "  (3865, 5229)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(tf[\"text\"].tolist())\n",
    "print(vec.get_feature_names_out())\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse-my459",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
