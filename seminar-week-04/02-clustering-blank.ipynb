{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Notebook 2.2: Clustering\n",
    "\n",
    "**LSE MY459: Computational Text Analysis and Large Language Models** (WT 2026)\n",
    "\n",
    "**Ryan Hübert**\n",
    "\n",
    "This notebook covers the vector space approach and $k$-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory management\n",
    "\n",
    "We begin with some directory management to specify the file path to the folder on your computer where you wish to store data for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sdir = os.path.join(os.path.expanduser(\"~\"), \"LSE-MY459-WT26\", \"SeminarWeek04\") # or whatever path you want\n",
    "if not os.path.exists(sdir):\n",
    "    os.mkdir(sdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the DFM\n",
    "\n",
    "We need to load the DFM we created in the last notebook. We start by reading the sparse array object we saved as an `.npz` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1959, 6236)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse_dfm_file = os.path.join(sdir, 'guardian-dfm.npz')\n",
    "if os.path.exists(sparse_dfm_file):\n",
    "    dfm = sparse.load_npz(sparse_dfm_file)\n",
    "else:\n",
    "    raise ValueError(\"You must create the DFM using the previous notebook before proceeding!\")\n",
    "\n",
    "dfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the list of features (the vocabulary), which remember is not included with the sparse array data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_file = os.path.join(sdir, 'guardian-dfm-features.txt')\n",
    "vocabulary = open(features_file, mode = \"r\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distance and similarity\n",
    "\n",
    "Before we look at $k$-means clustering, let's examine how to calculate distance and similarity between documents. First, we can calculate the Euclidean and Manhattan distances between documents using the formula from lecture. Let's calculate these two distance metrics between document 0 and document 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n",
      "255.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "ed = np.sqrt((dfm[0] - dfm[1]).power(2)).sum()\n",
    "\n",
    "md = np.abs(dfm[0] - dfm[1]).sum()\n",
    "\n",
    "print(ed)\n",
    "\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a convenient function available in `sklearn` for calculating distance. This function allows you to choose which metric you want to use, and it allows you to calculate distance between all documents (returning a matrix of pairwise distances). Let's calculate Euclidean and Manhattan distance between the first five documents. Note that Manhattan distance is called `cityblock` in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., 255.],\n",
       "       [255.,   0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "pairwise_distances(dfm[0:2], metric=\"euclidean\")\n",
    "pairwise_distances(dfm[0:2], metric=\"cityblock\") #这个就是曼哈顿距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the cosine similarity between two documents. For example, let's look at document 0 and 1. As we can see, they are not very similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01239644]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(dfm[0], dfm[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means Clustering\n",
    "\n",
    "First, we will weight the DFM using TF-IDF weighting. Note that, by default, `TfidfTransformer` applies a normalisation to ensure that all of the vectors in the DFM have the same magnitude. The default is to apply the L2 norm, which is another way of saying the vector for each row is normalised by its vector magnitude. This is exactly what we did when we computed cosine similarity in week 3 lecture. This normalisation removes differences due purely to document length, allowing clustering to focus on differences in word composition rather than scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 231987 stored elements and shape (1959, 6236)>\n",
      "  Coords\tValues\n",
      "  (0, 39)\t0.05256121464142173\n",
      "  (0, 100)\t0.16583917900223952\n",
      "  (0, 119)\t0.1392357025124698\n",
      "  (0, 144)\t0.15547711052686594\n",
      "  (0, 165)\t0.04251758740448491\n",
      "  (0, 176)\t0.05420018460240871\n",
      "  (0, 346)\t0.04634991464746871\n",
      "  (0, 427)\t0.2088535537687047\n",
      "  (0, 446)\t0.059869919567261336\n",
      "  (0, 613)\t0.05334994004923864\n",
      "  (0, 648)\t0.04987690007060491\n",
      "  (0, 677)\t0.051825703508955316\n",
      "  (0, 709)\t0.06742934699399131\n",
      "  (0, 768)\t0.11609607265961656\n",
      "  (0, 891)\t0.036356895150812286\n",
      "  (0, 926)\t0.03960974894494444\n",
      "  (0, 1048)\t0.06560746375653825\n",
      "  (0, 1054)\t0.05334994004923864\n",
      "  (0, 1239)\t0.06479924603171734\n",
      "  (0, 1367)\t0.036530468286103866\n",
      "  (0, 1406)\t0.06742934699399131\n",
      "  (0, 1418)\t0.07235867345844732\n",
      "  (0, 1443)\t0.04075021455486167\n",
      "  (0, 1534)\t0.04565503325965483\n",
      "  (0, 1567)\t0.060377264460157294\n",
      "  :\t:\n",
      "  (1958, 4894)\t0.03557551795133242\n",
      "  (1958, 4954)\t0.07518884265745372\n",
      "  (1958, 5019)\t0.14993714597065125\n",
      "  (1958, 5043)\t0.5763271517662182\n",
      "  (1958, 5044)\t0.064399721375835\n",
      "  (1958, 5092)\t0.05442721838842425\n",
      "  (1958, 5109)\t0.04043004455441167\n",
      "  (1958, 5215)\t0.03603112650065312\n",
      "  (1958, 5225)\t0.11853160558372411\n",
      "  (1958, 5277)\t0.048576366857007544\n",
      "  (1958, 5312)\t0.042346789010463\n",
      "  (1958, 5426)\t0.0456384696307792\n",
      "  (1958, 5468)\t0.056979452519705945\n",
      "  (1958, 5491)\t0.09641437191887324\n",
      "  (1958, 5492)\t0.04700694953229521\n",
      "  (1958, 5529)\t0.04750495490962845\n",
      "  (1958, 5581)\t0.041636231612657275\n",
      "  (1958, 5609)\t0.0541076368926966\n",
      "  (1958, 5700)\t0.05618611296183757\n",
      "  (1958, 5755)\t0.05657543532363946\n",
      "  (1958, 5898)\t0.07478863717055366\n",
      "  (1958, 6018)\t0.041441586521673554\n",
      "  (1958, 6020)\t0.0372629649731229\n",
      "  (1958, 6047)\t0.035744107027394854\n",
      "  (1958, 6168)\t0.03687830616403287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "dfm_tfidf = transformer.fit_transform(dfm)\n",
    "print(dfm_tfidf)\n",
    "\n",
    "#标准化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will \"set up\" our $k$-means clustering exercise. For now, let's try to find 30 clusters and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 9, ..., 2, 5, 9], shape=(1959,), dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 10 #需要仔细考虑，cohenrente cluster\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "labels = kmeans.fit_predict(dfm_tfidf)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What objects can we extract from this? We are interested in each document $i$'s cluster assignment $\\widehat{\\boldsymbol{\\pi}}_i$, as well as each cluster $k$'s \"word usage\" as represented by the centroid $\\widehat{\\boldsymbol{\\mu}}_k$. Where can we extract those quantities from the `kmeans` and/or `labels` objects? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster assignment \n",
    "\n",
    "This gives you the cluster assignments for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 9 ... 2 5 9]\n"
     ]
    }
   ],
   "source": [
    "cluster_assignments = labels\n",
    "print(cluster_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we see that document 0 is in cluster 3. This means: $\\widehat{\\boldsymbol{\\pi}}_0 = (0,0,1,0,0,0,0,0,0,0)$. (Remember: Python uses zero-indexing.) Let's now look at the distribution of documents across all clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({np.int32(5): 701,\n",
       "         np.int32(9): 342,\n",
       "         np.int32(4): 221,\n",
       "         np.int32(3): 161,\n",
       "         np.int32(6): 111,\n",
       "         np.int32(2): 109,\n",
       "         np.int32(8): 95,\n",
       "         np.int32(1): 94,\n",
       "         np.int32(7): 74,\n",
       "         np.int32(0): 51})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster centroid feature use\n",
    "\n",
    "The following gives you a $K \\times J$ matrix (in our case $10 \\times 6236$) of cluster centroids, $\\widehat{\\boldsymbol{\\mu}}$. Each row is a specific cluster $k$'s \"average document\", which we can interpret as representing the cluster's prototypical word usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.00068003 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.00039359 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.00213422 0.         ... 0.00296132 0.         0.        ]]\n",
      "(10, 6236)\n"
     ]
    }
   ],
   "source": [
    "mu = kmeans.cluster_centers_\n",
    "\n",
    "print(mu)\n",
    "print(mu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a specific cluster's centroid by extracting a row of this matrix, such as cluster 0 (the \"first\" cluster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], shape=(6236,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster, we can use the cluster's row in `mu` to find the top words of that cluster. More specifically, the words used the most in the cluster's centroid. Consider cluster 0. First, let's figure out which of the elements of $\\boldsymbol{\\mu}_0$ represent the 6 most used words in this cluster's centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1704    0.017439\n",
       "5977    0.014408\n",
       "4922    0.014037\n",
       "2316    0.013395\n",
       "3826    0.013312\n",
       "4301    0.012764\n",
       "32      0.012309\n",
       "359     0.011724\n",
       "4354    0.011707\n",
       "297     0.011084\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_top = 10\n",
    "cluster = 5\n",
    "\n",
    "import pandas as pd\n",
    "tf = pd.Series(mu[cluster])\n",
    "tf = tf.nlargest(num_top)\n",
    "tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the _tokens_ that correspond to these `mu[0]` values, and then bind it as a column to `tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drug',\n",
       " 'violenc',\n",
       " 'sexual',\n",
       " 'girl',\n",
       " 'obama',\n",
       " 'prison',\n",
       " 'abus',\n",
       " 'australian',\n",
       " 'protest',\n",
       " 'arrest']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocabulary[x] for x in tf.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can do this for each of the clusters to get a general sense for what they are about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating clusters discriminating words\n",
    "\n",
    "We can also calculate the discriminating words of each cluster using `sklearn`'s function `chi2`, which calculate the Pearson's chi2 statistic from lecture 3. Let's start by doing it for one cluster to see the basic process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a bar chart depicting the top 10 most discriminating words for cluster 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do it for all clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse-my459",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
