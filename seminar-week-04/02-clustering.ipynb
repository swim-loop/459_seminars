{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Notebook 2.2: Clustering\n",
    "\n",
    "**LSE MY459: Computational Text Analysis and Large Language Models** (WT 2026)\n",
    "\n",
    "**Ryan HÃ¼bert**\n",
    "\n",
    "This notebook covers the vector space approach and $k$-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory management\n",
    "\n",
    "We begin with some directory management to specify the file path to the folder on your computer where you wish to store data for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sdir = os.path.join(os.path.expanduser(\"~\"), \"LSE-MY459-WT26\", \"SeminarWeek04\") # or whatever path you want\n",
    "if not os.path.exists(sdir):\n",
    "    os.mkdir(sdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the DFM\n",
    "\n",
    "We need to load the DFM we created in the last notebook. We start by reading the sparse array object we saved as an `.npz` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse_dfm_file = os.path.join(sdir, 'guardian-dfm.npz')\n",
    "if os.path.exists(sparse_dfm_file):\n",
    "    dfm = sparse.load_npz(sparse_dfm_file)\n",
    "else:\n",
    "    raise ValueError(\"You must create the DFM using the previous notebook before proceeding!\")\n",
    "\n",
    "dfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the list of features (the vocabulary), which remember is not included with the sparse array data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_file = os.path.join(sdir, 'guardian-dfm-features.txt')\n",
    "vocabulary = open(features_file, mode = \"r\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating distance and similarity\n",
    "\n",
    "Before we look at $k$-means clustering, let's examine how to calculate distance and similarity between documents. First, we can calculate the Euclidean and Manhattan distances between documents using the formula from lecture. Let's calculate these two distance metrics between document 0 and document 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ed = np.sqrt(((dfm[0] - dfm[1]).power(2)).sum())\n",
    "print(ed)\n",
    "\n",
    "md = np.abs(dfm[0] - dfm[1]).sum()\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a convenient function available in `sklearn` for calculating distance. This function allows you to choose which metric you want to use, and it allows you to calculate distance between all documents (returning a matrix of pairwise distances). Let's calculate Euclidean and Manhattan distance between the first five documents. Note that Manhattan distance is called `cityblock` in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "edist = pairwise_distances(dfm[0:5], metric=\"euclidean\")\n",
    "print(edist)\n",
    "\n",
    "mdist = pairwise_distances(dfm[0:5], metric=\"cityblock\")\n",
    "print(mdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the cosine similarity between two documents. For example, let's look at document 0 and 1. As we can see, they are not very similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cs = cosine_similarity(dfm[0], dfm[1])  # cosine similarity\n",
    "print(cs)\n",
    "np.arccos(cs)                           # radians between documents\n",
    "np.degrees(np.arccos(cs))               # degrees between documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means Clustering\n",
    "\n",
    "First, we will weight the DFM using TF-IDF weighting. Note that, by default, `TfidfTransformer` applies a normalisation to ensure that all of the vectors in the DFM have the same magnitude. The default is to apply the L2 norm, which is another way of saying the vector for each row is normalised by its vector magnitude. This is exactly what we did when we computed cosine similarity in week 3 lecture. This normalisation removes differences due purely to document length, allowing clustering to focus on differences in word composition rather than scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "dfm_tfidf = transformer.fit_transform(dfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will \"set up\" our $k$-means clustering exercise. For now, let's try to find 30 clusters and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K = 10\n",
    "kmeans = KMeans(n_clusters=K, random_state=42) \n",
    "labels = kmeans.fit_predict(dfm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What objects can we extract from this? We are interested in each document $i$'s cluster assignment $\\widehat{\\boldsymbol{\\pi}}_i$, as well as each cluster $k$'s \"word usage\" as represented by the centroid $\\widehat{\\boldsymbol{\\mu}}_k$. Where can we extract those quantities from the `kmeans` and/or `labels` objects? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster assignment \n",
    "\n",
    "This gives you the cluster assignments for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = labels\n",
    "print(cluster_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we see that document 0 is in cluster 3. This means: $\\widehat{\\boldsymbol{\\pi}}_0 = (0,0,1,0,0,0,0,0,0,0)$. (Remember: Python uses zero-indexing.) Let's now look at the distribution of documents across all clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cf = pd.Series(labels).value_counts()\n",
    "cf = pd.concat([cf, cf / cf.sum()], axis = 1, keys=[\"doc_count\", \"doc_prop\"])\n",
    "cf = cf.sort_index()\n",
    "cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster centroid feature use\n",
    "\n",
    "The following gives you a $K \\times J$ matrix (in our case $10 \\times 6236$) of cluster centroids, $\\widehat{\\boldsymbol{\\mu}}$. Each row is a specific cluster $k$'s \"average document\", which we can interpret as representing the cluster's prototypical word usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = kmeans.cluster_centers_\n",
    "print(mu)\n",
    "print(mu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a specific cluster's centroid by extracting a row of this matrix, such as cluster 0 (the \"first\" cluster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster, we can use the cluster's row in `mu` to find the top words of that cluster. More specifically, the words used the most in the cluster's centroid. Consider cluster 0. First, let's figure out which of the elements of $\\boldsymbol{\\mu}_0$ represent the 6 most used words in this cluster's centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many \"top words\" do we want?\n",
    "num_top_feats = 6\n",
    "\n",
    "# Convert a row of mu to a Series object \n",
    "tf = pd.Series(mu[0]) \n",
    "# Get the top features (along with indexes)\n",
    "tf = tf.nlargest(num_top_feats)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the _tokens_ that correspond to these `mu[0]` values, and then bind it as a column to `tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = pd.Series([vocabulary[x] for x in tf.index], index=tf.index)\n",
    "tf = pd.concat([tf, top_words], axis=1, keys=[\"mu0_j\", \"j\"])\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can do this for each of the clusters to get a general sense for what they are about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pd.DataFrame(mu) \n",
    "tf = tf.apply(pd.Series.nlargest, n=num_top_feats, axis=1)\n",
    "tf = tf.reset_index().melt(id_vars=\"index\", var_name=\"j\", value_name=\"mu_kj\").rename(columns={\"index\": \"cluster\"})\n",
    "tf = tf.dropna(subset=[\"mu_kj\"])\n",
    "tf = tf.sort_values([\"cluster\", \"mu_kj\"], ascending=[True, False])\n",
    "tf = tf.reset_index(drop=True)\n",
    "tf[\"feature\"] = [vocabulary[x] for x in tf[\"j\"]]\n",
    "\n",
    "top_words = tf.groupby(\"cluster\")[\"feature\"].apply(lambda s: \", \".join(s.astype(str)))\n",
    "\n",
    "for i,r in top_words.items():\n",
    "    print(f\"Cluster {i} top words: {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating clusters discriminating words\n",
    "\n",
    "We can also calculate the discriminating words of each cluster using `sklearn`'s function `chi2`, which calculate the Pearson's chi2 statistic from lecture 3. Let's start by doing it for one cluster to see the basic process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "target = cluster_assignments == 0  # cluster 0 versus all other clusters\n",
    "scores, pvals = chi2(dfm, target)  # chi2 against null hypothesis (for all features at once)\n",
    "\n",
    "# Now let's format nicely\n",
    "disc_words = pd.DataFrame({\"cluster\": 0, \"feature\": vocabulary, \"chi2\" : scores, \"pval\" : pvals})\n",
    "disc_words = disc_words.sort_values(\"chi2\", ascending=False)\n",
    "print(disc_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a bar chart depicting the top 10 most discriminating words for cluster 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top = disc_words.nlargest(10, \"chi2\").sort_values(\"chi2\")\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(top[\"feature\"], top[\"chi2\"])\n",
    "plt.xlabel(\"Chi-square statistic\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title(\"Most indicative features of cluster 0\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do it for all clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwf = pd.DataFrame()\n",
    "\n",
    "for cluster in range(K):\n",
    "    target = cluster_assignments == cluster\n",
    "    scores, pvals = chi2(dfm, target)\n",
    "    disc_words = pd.DataFrame({\"cluster\": cluster, \"feature\": vocabulary, \"chi2\" : scores, \"pval\" : pvals})\n",
    "    disc_words = disc_words.sort_values(\"chi2\", ascending=False).iloc[0:num_top_feats,:]\n",
    "    disc_words = disc_words.loc[disc_words[\"pval\"] < 0.05,:]\n",
    "    dwf = pd.concat([dwf, disc_words], axis = 0)\n",
    "\n",
    "disc_words = dwf.groupby(\"cluster\")[\"feature\"].apply(lambda s: \", \".join(s.astype(str)))\n",
    "\n",
    "for i,r in disc_words.items():\n",
    "    print(f\"Cluster {i} most discriminating words: {r}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse-my459",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
