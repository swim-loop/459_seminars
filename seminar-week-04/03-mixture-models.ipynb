{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Notebook 2.3: Mixture Models\n",
    "\n",
    "**LSE MY459: Computational Text Analysis and Large Language Models** (WT 2026)\n",
    "\n",
    "**Ryan Hübert**\n",
    "\n",
    "This notebook covers mixture models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DGP of a simple mixture model\n",
    "\n",
    "In lecture, we looked at a probabilistic model of a DGP in which documents are assigned to clusters; clusters differ in their word usage; and documents' token counts are drawn from a multinomial distribution. Let's create a simulated data set of a DFM that would be generated by such a model. We will use tools available in `numpy` to do the sampling and the matrix algebra. We'll also set the seed by creating a random number generator called `rng` with a seed of `2026`. Warning: if you run the following cells out of order (or re-run them more than once), you will no longer get the same numbers. Keep this in mind when you compare your results with what is written in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(2026) # set seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a simple example of a corpus with a vocabulary that has 5 distinct types (i.e., pre-processed unigrams), which is organised into a latent structure of 3 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"cat\", \"dog\", \"rat\", \"porcupine\", \"fox\"]\n",
    "J = len(vocabulary)\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this structure, each document $i$ will be assigned to one of the three clusters, and each cluster $k$ will have a parameter $\\boldsymbol{\\mu}_k$ which will have five elements in it---one for each of the features. Recall that in the model from lecture, both $\\boldsymbol{\\pi}_i$ and $\\boldsymbol{\\mu}_k$ are drawn from distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster feature probabilities \n",
    "\n",
    "Let's first examine $\\boldsymbol{\\mu}_k$, which under the model from class is drawn from a Dirichlet distribution with a parameter $\\boldsymbol{\\eta}$ that is a $J$ length vector. Let's draw a $\\boldsymbol{\\mu}_k$, using $\\boldsymbol{\\eta} = (1,1,1,1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04242819, 0.36086379, 0.12227104, 0.19567413, 0.27876284])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = [1, 1, 1, 1, 1]\n",
    "muk = rng.dirichlet(eta)\n",
    "muk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This randomly sampled $\\boldsymbol{\\mu}_k$ tells you how words are used in cluster $k$. Given that token counts will be sampled from a multinomial (details below), you can interpret this vector as saying: \"When a token is drawn for a document, there is a 0.042 probability that token will be \"cat\", a 0.361 probability that token will be \"dog\", and so on.\"\n",
    "\n",
    "Of course, we need to draw a $\\boldsymbol{\\mu}_k$ for every cluster $k \\in \\{1,2,...,K\\}$. In this case, we need to draw $\\boldsymbol{\\mu}_1$, $\\boldsymbol{\\mu}_2$ and $\\boldsymbol{\\mu}_3$ since there are three clusters. We can do this by sampling as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27266358, 0.32792981, 0.00474568, 0.23269553, 0.1619654 ],\n",
       "       [0.261513  , 0.43068886, 0.21252974, 0.0693821 , 0.0258863 ],\n",
       "       [0.17307401, 0.08741606, 0.29401094, 0.11437959, 0.33111939]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = [1, 1, 1, 1, 1]\n",
    "mu = rng.dirichlet(eta, size=K) # sample K mu_k's\n",
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mu` object is a $K \\times J$ matrix, where each row is one of the cluster's $\\boldsymbol{\\mu}_k$ vector. We can also do a little formatting to make it easier to see that these columns correspond to types in the vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>rat</th>\n",
       "      <th>porcupine</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.272664</td>\n",
       "      <td>0.327930</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.232696</td>\n",
       "      <td>0.161965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.261513</td>\n",
       "      <td>0.430689</td>\n",
       "      <td>0.212530</td>\n",
       "      <td>0.069382</td>\n",
       "      <td>0.025886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.173074</td>\n",
       "      <td>0.087416</td>\n",
       "      <td>0.294011</td>\n",
       "      <td>0.114380</td>\n",
       "      <td>0.331119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat       dog       rat  porcupine       fox\n",
       "0  0.272664  0.327930  0.004746   0.232696  0.161965\n",
       "1  0.261513  0.430689  0.212530   0.069382  0.025886\n",
       "2  0.173074  0.087416  0.294011   0.114380  0.331119"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(mu, columns = vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this matrix, you can already start to see some patterns that will emerge when we generate documents from this model. For example, documents in cluster 2 will be about twice as likely to have the word \"fox\" as documents in cluster 0 (Note: we are using Python indexing, which starts at zero!). That's because the probability that any given token will be \"fox\" is 0.33 in cluster 2 and 0.16 in cluster 0. You can also see that the top most used feature in clusters 0 and 1 is \"dog\", the top most used feature in cluster 2 is \"fox\".\n",
    "\n",
    "These $\\boldsymbol{\\mu}_k$ vectors are drawn from a Dirichlet distribution, where its parameter $\\boldsymbol{\\eta}$ controls how \"concentrated\" word use is across the clusters. Let's see this by looking at the results from using different values for the prior. First, we'll again use a symmetric vector, but we'll have all values be much smaller than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>rat</th>\n",
       "      <th>porcupine</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.086679</td>\n",
       "      <td>0.882851</td>\n",
       "      <td>0.017356</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.013110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.776625</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.024049</td>\n",
       "      <td>0.199145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.076120</td>\n",
       "      <td>0.771987</td>\n",
       "      <td>0.151859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat       dog       rat  porcupine       fox\n",
       "0  0.086679  0.882851  0.017356   0.000004  0.013110\n",
       "1  0.000001  0.776625  0.000179   0.024049  0.199145\n",
       "2  0.000026  0.000007  0.076120   0.771987  0.151859"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta2 = [0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "pd.DataFrame(rng.dirichlet(eta2, size=K), columns = vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that word use is now much more concentrated within a cluster. For example, in both clusters 0 and 1, \"dog\" is the most probable word to be used, and by a lot more than the previous example. Next let's use a symmetric vector, but we'll have all values be much greater than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>rat</th>\n",
       "      <th>porcupine</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180035</td>\n",
       "      <td>0.211119</td>\n",
       "      <td>0.214582</td>\n",
       "      <td>0.194129</td>\n",
       "      <td>0.200135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194010</td>\n",
       "      <td>0.185286</td>\n",
       "      <td>0.177185</td>\n",
       "      <td>0.232969</td>\n",
       "      <td>0.210551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.208684</td>\n",
       "      <td>0.191149</td>\n",
       "      <td>0.193428</td>\n",
       "      <td>0.214673</td>\n",
       "      <td>0.192065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat       dog       rat  porcupine       fox\n",
       "0  0.180035  0.211119  0.214582   0.194129  0.200135\n",
       "1  0.194010  0.185286  0.177185   0.232969  0.210551\n",
       "2  0.208684  0.191149  0.193428   0.214673  0.192065"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta3 = [100, 100, 100, 100, 100]\n",
    "pd.DataFrame(rng.dirichlet(eta3, size=K), columns = vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see that word use is now much less concentrated within a cluster. In all the clusters, the probabilities of each word are much more similar, so document word use is more \"random\" than before. Finally, let's see what happens when we use an asymmetric $\\boldsymbol{\\eta}$, such as $\\boldsymbol{\\eta} = (10,1,1,1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>rat</th>\n",
       "      <th>porcupine</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.416221</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>0.045816</td>\n",
       "      <td>0.058462</td>\n",
       "      <td>0.465363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.640488</td>\n",
       "      <td>0.041092</td>\n",
       "      <td>0.178232</td>\n",
       "      <td>0.036490</td>\n",
       "      <td>0.103698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.624992</td>\n",
       "      <td>0.051490</td>\n",
       "      <td>0.269255</td>\n",
       "      <td>0.013534</td>\n",
       "      <td>0.040729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat       dog       rat  porcupine       fox\n",
       "0  0.416221  0.014137  0.045816   0.058462  0.465363\n",
       "1  0.640488  0.041092  0.178232   0.036490  0.103698\n",
       "2  0.624992  0.051490  0.269255   0.013534  0.040729"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta4 = [10, 1, 1, 1, 1]\n",
    "pd.DataFrame(rng.dirichlet(eta4, size=K), columns = vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that in all three clusters, \"cat\" gets a lot of weight, reflecting the asymmetric prior. However, due to randomness (after all, these are still random draws), you still see the first document has a top feature that is not \"cat\" despite the lopsided $\\boldsymbol{\\eta}$. \n",
    "\n",
    "Of course, in very large samples (i.e., large numbers of clusters), you will see the average probability of the \"cat\" feature will dominate, as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat          0.701344\n",
      "dog          0.080072\n",
      "rat          0.073954\n",
      "porcupine    0.071846\n",
      "fox          0.072784\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "q = pd.DataFrame(rng.dirichlet(eta4, size=1000), columns = vocabulary)\n",
    "print(q.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document cluster assignment\n",
    "\n",
    "Under this model, $\\boldsymbol{\\pi}_i$ is drawn from a multinomial distribution with two parameters: 1 and $\\boldsymbol{\\alpha}$, where $\\boldsymbol{\\alpha}$, is a $K$-length vector giving the probability that a document will be assigned to each of the clusters. For example, let's consider a scenario where $\\boldsymbol{\\alpha} = (0.20, 0.40, 0.40)$, so that each document has a 0.2 probability of being assigned to cluster 0, a 0.4 probability of being assigned to cluster 1, and a 0.4 probability of being assigned to cluster 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.2, 0.4, 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a hypothetical document $i$, let's draw that document's cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pii = rng.multinomial(1, alpha)\n",
    "pii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that the document is assigned to cluster 2. Since $\\boldsymbol{\\alpha}$ controls the probability that documents will be assigned to clusters, we can again see that in a very large sample, roughly 20% of documents will be in cluster 0, 40% in clusters 1 and 2, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster0    0.1905\n",
      "cluster1    0.4010\n",
      "cluster2    0.4085\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "q = pd.DataFrame(rng.multinomial(1, alpha, size = 10000), columns = [\"cluster0\", \"cluster1\", \"cluster2\"])\n",
    "print(q.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a document\n",
    "\n",
    "Now that we have each cluster's $\\boldsymbol{\\mu}_k$, _and_ we know which cluster our hypothetical document $i$ belongs to, we can randomly sample a vector of word counts for the hypothetical document $i$. According to the model, we use a multinomial distribution with two parameters: $M_i$ (document length) and $\\boldsymbol{\\pi}_i\\boldsymbol{\\mu}$. Due to matrix algebra (watch the indexing!) and since we saw the document was assigned to cluster 1, $\\boldsymbol{\\pi}_i\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1$, as you can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.261513  , 0.43068886, 0.21252974, 0.0693821 , 0.0258863 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pii @ mu # This is the same as row 1 of the mu object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can generate token counts for this hypothetical document, assuming the document has 10 tokens in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 4, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.multinomial(10, pii @ mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a full DFM\n",
    "\n",
    "Above, we simulated a single document. But can we simulate a full DFM? Yes! First, we need to define how many documents we want in our simulated DFM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how big we want our DFM, we can draw $\\boldsymbol{\\pi}_i$ for each of the documents, and then draw token counts for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>rat</th>\n",
       "      <th>porcupine</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cat  dog  rat  porcupine  fox\n",
       "0      1    5    4          0    0\n",
       "1      3    3    1          2    1\n",
       "2      1    0    3          0    6\n",
       "3      2    1    3          2    2\n",
       "4      5    2    0          2    1\n",
       "..   ...  ...  ...        ...  ...\n",
       "995    0    1    3          1    5\n",
       "996    2    1    3          1    3\n",
       "997    3    4    2          0    1\n",
       "998    2    1    2          1    4\n",
       "999    1    4    3          2    0\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = rng.multinomial(1, alpha, size=N)  # Sample a pi_i for every document\n",
    "M = [10] * N                            # Make all documents 10 tokens (for simplicity)\n",
    "dfm = pd.DataFrame(rng.multinomial(10, pi @ mu), columns=vocabulary)\n",
    "dfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run $k$-means on simulated data\n",
    "\n",
    "We now have a simulated DFM where token usage in each document depends on cluster assignment (`pi`), and the clusters' word usage probabilities (`mu`). Now, let's pretend that this is a real dataset, and try to run $k$-means clustering on it to see if we can recover the cluster assignments we simulated above. Recall that each document's cluster assignment is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0]], shape=(1000, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make this a bit easier for us to see by creating a `DataFrame` object where each row gives the known cluster (created from `pi`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>known_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   known_cluster\n",
       "0              1\n",
       "1              1\n",
       "2              2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_assignment = pd.DataFrame(pi.argmax(axis=1), columns=[\"known_cluster\"])\n",
    "cluster_assignment[0:3] # examine the first 3 to see they match with pi above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run $k$-means clustering, using the process from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=K, random_state=42) \n",
    "labels = kmeans.fit_predict(dfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add our _estimated_ cluster to the `cluster_assignment` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>known_cluster</th>\n",
       "      <th>estimated_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     known_cluster  estimated_cluster\n",
       "0                1                  0\n",
       "1                1                  0\n",
       "2                2                  2\n",
       "3                2                  1\n",
       "4                0                  0\n",
       "..             ...                ...\n",
       "995              2                  2\n",
       "996              2                  1\n",
       "997              1                  0\n",
       "998              2                  2\n",
       "999              1                  0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_assignment = pd.concat([cluster_assignment, pd.Series(labels)], axis=1)\n",
    "cluster_assignment.columns = [\"known_cluster\", \"estimated_cluster\"]\n",
    "cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding note: $k$-means will assign _arbitrary_ labels to each cluster, so we need to match up the labels from our known clusters to our estimated clusters. The following code will do this using the [Hungarian algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm), although the details are not important. What is important to know is that k-means won't necessarily call the same clusters the same thing as you had called them when you created them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def relabel_clusters(known_clusters, estimated_clusters):\n",
    "    cm = confusion_matrix(known_clusters, estimated_clusters)     # rows=true, cols=pred\n",
    "    r, c = linear_sum_assignment(-cm)\n",
    "    mapping = dict(zip(c, r))\n",
    "    return estimated_clusters.apply(lambda x : mapping[x])\n",
    "\n",
    "cluster_assignment[\"estimated_cluster\"] = relabel_clusters(cluster_assignment[\"known_cluster\"], cluster_assignment[\"estimated_cluster\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have relabelled, let's see how many documents $k$-means accurately assigned to a cluster (relative to the actual cluster assignment from our simulated data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cluster_assignment[\"known_cluster\"]==cluster_assignment[\"estimated_cluster\"])/len(cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-means correctly recovered 58% of the clusters we assigned above. That's okay, but not _great_. Will we improve things by weighting and normalising our simulated DFM? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.682"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "dfm_tfidf = transformer.fit_transform(dfm)\n",
    "\n",
    "kmeans = KMeans(n_clusters=K, random_state=42) \n",
    "labels = kmeans.fit_predict(dfm_tfidf)\n",
    "\n",
    "cluster_assignment = pd.concat([cluster_assignment, pd.Series(labels)], axis=1)\n",
    "cluster_assignment.columns = [\"known_cluster\", \"estimated_cluster1\", \"estimated_cluster2\"]\n",
    "\n",
    "cluster_assignment[\"estimated_cluster2\"] = relabel_clusters(cluster_assignment[\"known_cluster\"], cluster_assignment[\"estimated_cluster2\"])\n",
    "\n",
    "sum(cluster_assignment[\"known_cluster\"]==cluster_assignment[\"estimated_cluster2\"])/len(cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this does better! But you may be able to do even better with other clustering methods. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse-my459",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
