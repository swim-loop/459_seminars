{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Notebook 2.1: Making a DFM\n",
    "\n",
    "**LSE MY459: Computational Text Analysis and Large Language Models** (WT 2026)\n",
    "\n",
    "**Ryan HÃ¼bert**\n",
    "\n",
    "This notebook covers creating a DFM using a corpus of news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory management\n",
    "\n",
    "We begin with some directory management to specify the file path to the folder on your computer where you wish to store data for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sdir = os.path.join(os.path.expanduser(\"~\"), \"LSE-MY459-WT26\", \"SeminarWeek04\") # or whatever path you want\n",
    "if not os.path.exists(sdir):\n",
    "    os.mkdir(sdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by loading a corpus of news articles published in the Guardian during 2016. This corpus was sourced from the `{quanteda}` R package, see: <https://tutorials.quanteda.io/machine-learning/topicmodel/>. You can get a `.csv` version of the file from the `data` repo in the course GitHub. The following code chunk will download this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the remote file?\n",
    "rfile = \"https://raw.githubusercontent.com/lse-my459/data/master/corpus_guardian_2016.csv\"\n",
    "\n",
    "# Where will we store the local copy of it?\n",
    "lfile = os.path.join(sdir, os.path.basename(rfile))\n",
    "\n",
    "# Check if you have the file yet and if not, download it to correct location\n",
    "if not os.path.exists(lfile):\n",
    "    import requests         # only import module if needed\n",
    "    r = requests.get(rfile) # make GET request for the remote file\n",
    "    r.raise_for_status()    # raise exception if there's an HTTP error\n",
    "    # Write the raw bytes received from the server to the local file path\n",
    "    with open(lfile, \"wb\") as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the corpus\n",
    "\n",
    "In the next cell, we load and clean the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tf = pd.read_csv(lfile, dtype=\"object\")\n",
    "tf[\"tid\"] = tf[\"tid\"].astype(int)\n",
    "tf[\"datetime\"] = pd.to_datetime((tf[\"date\"] + \" \" + tf[\"edition\"].str.replace(\" GMT\", \"\")), errors=\"coerce\")\n",
    "tf = tf.loc[:, [\"tid\", \"pub\", \"datetime\", \"head\", \"texts\"]]\n",
    "tf = tf.sort_values(\"datetime\")\n",
    "tf.index = tf[\"tid\"]\n",
    "tf = tf.drop(\"tid\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will work through standard pre-processing steps. Keep in mind that we are looking at a corpus of news articles, and that may affect how we pre-process. Let's start by tokenising on white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[\"preprocessed\"] = tf[\"texts\"].str.split(r\"\\s+\")\n",
    "tf.loc[:,[\"texts\", \"preprocessed\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make all words lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x.lower() for x in doc_tokens])\n",
    "tf.loc[:,[\"texts\", \"preprocessed\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will clean up the text by removing extraneous punctuation, and dropping tokens we don't want. There is a lot of possible analyst discretion about how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Replace brackets\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [re.sub(r\"(^[\\[\\(\\{]|[\\]\\)\\}]$)\",\"\", x) for x in doc_tokens])\n",
    "# Keep only tokens that begin with a letter\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if re.search(r\"^[A-Za-z]\", x)])\n",
    "# Keep only tokens that have no numbers in them\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if not re.search(r\"[0-9]\", x)])\n",
    "# Remove all other punctuation, except dashes and apostrophes\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [re.sub(r'[^A-Za-z\\-\\']', '', x) for x in doc_tokens])\n",
    "# Drop strings with article publishing time info\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if not re.search(r\"^(.*\\-time|updated\\-.*|gmt|bst)$\", x)])\n",
    "# Drop empty strings\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if x != \"\"])\n",
    "tf.loc[:,[\"texts\", \"preprocessed\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove English stop words from this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = [x.lower() for x in stopwords.words('english')]\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [x for x in doc_tokens if not x in sw])\n",
    "tf.loc[:,[\"texts\", \"preprocessed\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use the Snowball stemmer to create equivalence classes of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import snowball\n",
    "sstemmer = snowball.SnowballStemmer(\"english\")\n",
    "tf[\"preprocessed\"] = tf[\"preprocessed\"].apply(lambda doc_tokens: [sstemmer.stem(x) for x in doc_tokens])\n",
    "tf.loc[:,[\"texts\", \"preprocessed\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, can apply a `Counter` to the preprocessed tokens in `tf` to get token counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tf[\"term_freqs\"] = tf[\"preprocessed\"].map(Counter)\n",
    "tf.loc[:,[\"texts\", \"preprocessed\", \"term_freqs\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a document feature matrix (DFM)\n",
    "\n",
    "We will now create a document feature matrix (DFM). We will use tools in `sklearn` to create a DFM in a sparse matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Step 1: initialise a DictVectorizer object, which we'll call `dv`\n",
    "dv = DictVectorizer()\n",
    "\n",
    "# Step 2: create the array from our data\n",
    "dfm = dv.fit_transform(tf[\"term_freqs\"].to_list())\n",
    "\n",
    "# Check the shape is correct before continuing\n",
    "dfm.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be sure to extract and preserve the vocabulary from the `DictVectorizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: be sure to keep the vocabulary for future reference\n",
    "vocabulary = dv.get_feature_names_out()\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming the DFM\n",
    "\n",
    "Notice that the DFM has over 34,000 features. This is a _very_ sparse DFM. We will want to reduce its size by \"trimming\" it to remove features (i.e., weight them by zero). We will use the recommendation in [this tutorial](https://tutorials.quanteda.io/machine-learning/topicmodel/) and only keep features that are both (1) in the top 20% of total term frequency, and (2) used in no more than 10% of documents. Note that we are trimming the DFM using a similar intuition as tf-idf weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Calculate TTF for each feature\n",
    "ttf = dfm.sum(axis = 0).A1\n",
    "## Calculate DF for each feature\n",
    "docf = (dfm > 0).sum(axis=0).A1\n",
    "## Define our cutoffs \n",
    "ttf_cutoff = np.quantile(ttf, 0.80)\n",
    "docf_cutoff = dfm.shape[0] * 0.1\n",
    "## Trim the DFM\n",
    "dfm = dfm[:, (ttf >= ttf_cutoff) & (docf <= docf_cutoff)]\n",
    "dfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: while we have trimmed the DFM, we have _not_ yet trimmed the corresponding `vocabulary` object, which we do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vocabulary[(ttf >= ttf_cutoff) & (docf <= docf_cutoff)]\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look at the top 20 features in this DFM to get a sense of how words are used in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = dfm.sum(axis = 0).A1\n",
    "top_features = pd.Series(top_features, index=vocabulary)\n",
    "top_features = top_features.astype(int)\n",
    "top_features = top_features.sort_values(ascending=False)\n",
    "top_features[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a wordcloud object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "\n",
    "# Initialise the wordcloud object as `wc`\n",
    "wc = wordcloud.WordCloud(width=800,\n",
    "                         height=400,\n",
    "                         background_color=\"white\",\n",
    "                         relative_scaling = 1,\n",
    "                         max_words=200, \n",
    "                         random_state=42)\n",
    "\n",
    "# Feed in the data from our `top_features` object\n",
    "wc = wc.generate_from_frequencies(top_features)\n",
    "\n",
    "# Don't use the default colours, and make blue\n",
    "wc = wc.recolor(color_func=lambda *args, **kwargs: (51,51,255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll plot the wordcloud object using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created this DFM, let's save it for further use. We'll also save the corpus file as well, so that we can access the texts if/when needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse_dfm_file = os.path.join(sdir, 'guardian-dfm.npz')\n",
    "sparse.save_npz(sparse_dfm_file, dfm)\n",
    "\n",
    "features_file = os.path.join(sdir, 'guardian-dfm-features.txt')\n",
    "with open(features_file, 'w') as f:\n",
    "    f.write(\"\\n\".join(vocabulary))\n",
    "\n",
    "corpus_file = os.path.join(sdir, 'guardian-corpus.csv')\n",
    "tf.iloc[:,0:4].to_csv(corpus_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse-my459",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
