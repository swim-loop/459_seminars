{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar Notebook 2.4: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "**LSE MY459: Computational Text Analysis and Large Language Models** (WT 2026)\n",
    "\n",
    "**Ryan HÃ¼bert**\n",
    "\n",
    "This notebook covers Latent Dirichlet Allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory management\n",
    "\n",
    "We begin with some directory management to specify the file path to the folder on your computer where you wish to store data for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sdir = os.path.join(os.path.expanduser(\"~\"), \"LSE-MY459-WT26\", \"SeminarWeek04\") # or whatever path you want\n",
    "if not os.path.exists(sdir):\n",
    "    os.mkdir(sdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the DFM\n",
    "\n",
    "We need to load the DFM we created in the last notebook. We start by reading the sparse array object we saved as an `.npz` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "\n",
    "sparse_dfm_file = os.path.join(sdir, 'guardian-dfm.npz')\n",
    "if os.path.exists(sparse_dfm_file):\n",
    "    dfm = sparse.load_npz(sparse_dfm_file)\n",
    "else:\n",
    "    raise ValueError(\"You must create the DFM using the previous notebook before proceeding!\")\n",
    "\n",
    "dfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the list of features (the vocabulary), which remember is not included with the sparse array data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_file = os.path.join(sdir, 'guardian-dfm-features.txt')\n",
    "vocabulary = open(features_file, mode = \"r\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "We will run LDA on our corpus of news articles. We'll estimate 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "K = 10\n",
    "lda = LatentDirichletAllocation(n_components=K, random_state=6541)\n",
    "lda = lda.fit(dfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was the case for $k$-means clustering, we are interested in each document $i$'s $\\widehat{\\boldsymbol{\\pi}}_i$, as well as each cluster $k$'s $\\widehat{\\boldsymbol{\\mu}}_k$. However in the context of topic modelling, they have different interpretations:\n",
    "\n",
    "- $\\widehat{\\boldsymbol{\\pi}}_i$ gives the proportion of document $i$ that corresponds to each topic\n",
    "- $\\widehat{\\boldsymbol{\\mu}}_k$ gives the word use for a topic $k$\n",
    "\n",
    "Where can we extract these important items from the `lda` object?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic assignment proportions \n",
    "\n",
    "We can extract each document's topic proportions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = lda.transform(dfm)\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we see that document 0 has the following proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document is 40% about topic 0, 24% about topic 3, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic feature probabilities (word use)\n",
    "\n",
    "Next, we need to examine what these topics are actually about. We begin by extracting a $K \\times J$ matrix (in our case $10 \\times 6236$), where each row gives a topic's $\\widehat{\\boldsymbol{\\mu}}_k$. In the DGP for LDA, this parameter controls the probabilities that each token in the vocabulary will be chosen when a token is assigned to that topic. The following code extracts this $\\widehat{\\boldsymbol{\\mu}}$ matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = lda.components_ / lda.components_.sum(axis=1, keepdims=True)\n",
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a specific topic's word usage by extracting a row of this matrix, such as topic 0 (the \"first\" topic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we can use the topic's row in `mu` to find the top words of that cluster. More specifically, the words used the most in the cluster's centroid. Consider cluster 0. First, let's figure out which of the elements of $\\boldsymbol{\\mu}_0$ represent the 6 most used words in this cluster's centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many \"top words\" do we want?\n",
    "num_top_feats = 6\n",
    "\n",
    "# Convert a row of mu to a Series object \n",
    "tf = pd.Series(mu[0]) \n",
    "# Get the top features (along with indexes)\n",
    "tf = tf.nlargest(num_top_feats)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know what each of the 10 topics are roughly about. So we can look at the top features for each topic $k$, as represented by the feature with the highest probability in $\\boldsymbol{\\mu}_k$. This is identical to what we did for $k$-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pd.DataFrame(mu) \n",
    "tf = tf.apply(pd.Series.nlargest, n=num_top_feats, axis=1)\n",
    "tf = tf.reset_index().melt(id_vars=\"index\", var_name=\"j\", value_name=\"mu_kj\").rename(columns={\"index\": \"topic\"})\n",
    "tf = tf.dropna(subset=[\"mu_kj\"])\n",
    "tf = tf.sort_values([\"topic\", \"mu_kj\"], ascending=[True, False])\n",
    "tf = tf.reset_index(drop=True)\n",
    "tf[\"feature\"] = [vocabulary[x] for x in tf[\"j\"]]\n",
    "\n",
    "top_words = tf.groupby(\"topic\")[\"feature\"].apply(lambda s: \", \".join(s.astype(str)))\n",
    "\n",
    "for i,r in top_words.items():\n",
    "    print(f\"Topic {i} top words: {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading documents\n",
    "\n",
    "Of course, if you want to really understand these topics, you will need to read a selection of documents corresponding to each one of the topics. Let's look at the five documents that have the highest proportion of tokens assigned to a topic. First, let's identify the top five documents for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_docs = 5\n",
    "zf = pd.DataFrame(pi) \n",
    "zf = zf.apply(pd.Series.nlargest, n=num_top_docs, axis=0)\n",
    "zf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the corpus documents and look at the top five documents in topic 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "topic = 0\n",
    "topic_idx = zf.iloc[:,topic].dropna().index\n",
    "\n",
    "corpus_file = os.path.join(sdir, 'guardian-corpus.csv')\n",
    "corpus = pd.read_csv(corpus_file)\n",
    "\n",
    "for i,r in corpus.iloc[topic_idx,:].iterrows():\n",
    "    print(r[\"datetime\"])\n",
    "    pprint(r[\"texts\"][0:210], width=80)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could, of course, do the same thing to review clusters as well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse-my459",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
